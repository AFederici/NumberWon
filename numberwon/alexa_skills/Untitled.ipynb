{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from search.collect_rss import collect\n",
    "from collections import defaultdict\n",
    "\n",
    "term = \"harry%20potter\"\n",
    "# resp = urllib.request.urlopen(\"https://api.wattpad.com:443/v4/stories\")\n",
    "# soup = BeautifulSoup(response.data, 'html.parser')\n",
    "\n",
    "site= \"https://www.wattpad.com/search/\" + term\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = Request(site,headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "\n",
    "\n",
    "def find_fanfiction(top = 1):\n",
    "    final = defaultdict(str)\n",
    "    for link in soup.find_all('a', 'on-result')[:top]: #if too slow just take 1st or second link\n",
    "        url = \"https://www.wattpad.com\" + link[\"href\"] + \"/parts\"\n",
    "        req = Request(url,headers=hdr)\n",
    "        page = urlopen(req)\n",
    "        soup2 = BeautifulSoup(page, \"lxml\")\n",
    "        #ul class=\"table-of-contents         \n",
    "        for link2 in soup2.find_all('ul', 'table-of-contents'):\n",
    "            z = link2.find_all('a', href=True)\n",
    "            listing = [ele[\"href\"] for ele in z]\n",
    "            d = defaultdict(BeautifulSoup)\n",
    "            for x in listing:\n",
    "                #print(url)\n",
    "                list2 = BeautifulSoup(urlopen(Request(\"https://www.wattpad.com\" + x, headers = hdr)), \"lxml\").find_all('p')\n",
    "                \n",
    "                paragraph = [str(j)[str(j).index('>')+1:str(j).rfind('<')] for j in list2 if \"data-p-id\" in str(j)]\n",
    "                #paragraph = \"\".join(paragraph)\n",
    "                final[term] += \"\\n\".join(paragraph)\n",
    "                print(final)\n",
    "        \n",
    "                #print(\"DONE boyz\" + str(len(paragraph)))\n",
    "                #print(paragraph)\n",
    "#                     string = \"\" + str(j)\n",
    "#                     if \"data-p-id\" in string:\n",
    "#                         print(string[string.index('>')+1:string.rfind('<')])\n",
    "#                         paragraph.append(string[string.index('>')+1:string.rfind('<')])\n",
    "                #print(paragraph)\n",
    "    print(len(final[term]))\n",
    "find_fanfiction()\n",
    "            #print(para)\n",
    "            #print(len(list2))\n",
    "            #print(list2)\n",
    "#             paragraph = [x for x in list2 if \"data-p-id\" in x]#list comprehension\n",
    "#             print(paragraph, \"paragraph\")\n",
    "        #print(d)\n",
    "        #print(listing, \"listing \")\n",
    "        #print(url)\n",
    "        \n",
    "#     l = link.find_all('a',  href=True)[0][\"href\"]\n",
    "#     print(l)\n",
    "#     p = collect(l, l[l.rfind(\"/\")+1:]+\".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\".\".join([str([\"a\",\"b\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(fanfiction_dict):\n",
    "    for key, value in fanfiction_dict:\n",
    "        lm = self.train_lm(value, n=10)\n",
    "        text = self.generate_text(lm, 10, n_letters=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "your_string = '\\xa0\"I\\'m sure it\\'s nothing bad, hun. \\xa0They probably want to tell us we\\'ve got the next Einstein on our hands!\" \\xa0'\n",
    "str(print(your_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/search')\n",
    "print(\"done\")\n",
    "from entityDatabase import entityDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entityDatabase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from search.searchEngine import MySearchEngine\n",
    "\n",
    "class entityDatabase:\n",
    "    def __init__(self):\n",
    "        self.ent_dict = defaultdict(list)\n",
    "        self.ent_dict2 = defaultdict(list)\n",
    "        self.engine = MySearchEngine()\n",
    "\n",
    "    def get_by_id(self, id):\n",
    "        return self.ent_dict[id]\n",
    "\n",
    "    def entize(self, pickle, dictionary):\n",
    "        #creates a dictionary where link = key, and value = list of entities\n",
    "        for key, val in pickle.items():\n",
    "            tokens = nltk.word_tokenize(val)\n",
    "            pos = nltk.pos_tag(tokens)\n",
    "            named_entities = nltk.ne_chunk(pos, binary = True)\n",
    "            for i in range(0, len(named_entities)):\n",
    "                ents = named_entities.pop()\n",
    "                if getattr(ents, 'label', None) != None and ents.label() == \"NE\":\n",
    "                    z = list(zip(*[ne for ne in ents]))[0]\n",
    "                    z = \" \".join(z)\n",
    "                    dictionary[key].append(z)\n",
    "        return dictionary\n",
    "\n",
    "    def entize2(self, pickle, dictionary):\n",
    "        counter = 0\n",
    "        for key, val in pickle.items():\n",
    "            tokens = nltk.word_tokenize(val)\n",
    "            pos = nltk.pos_tag(tokens)\n",
    "            named_entities = nltk.ne_chunk(pos, binary = True)\n",
    "            for i in range(0, len(named_entities)):\n",
    "                ents = named_entities.pop()\n",
    "                if getattr(ents, 'label', None) != None and ents.label() == \"NE\":\n",
    "                    z = list(zip(*[ne for ne in ents]))[0]\n",
    "                    z = (\" \".join(z), counter)\n",
    "                    dictionary[key].append(z)\n",
    "                counter += len(ents)\n",
    "        return dictionary\n",
    "\n",
    "    def searchNentity(self, qword):\n",
    "        if self.engine.query(qword) != []:\n",
    "            topdoc = self.engine.query(qword)[0][0]\n",
    "            return self.top_entity_pos(qword,self.engine.raw_text[topdoc]) #Megan's method\n",
    "        return None\n",
    "\n",
    "    def docsearch(self, qword):\n",
    "        if self.engine.query(qword) != []:\n",
    "            topdoc = self.engine.query(qword)[0][0]\n",
    "            raw = self.engine.raw_text[topdoc] #whole doc\n",
    "            return re.match(r'(?:[^.:;]+[.:;]){1}', raw).group().replace('\\n\\nFILE PHOTO', \"\") #first sentence\n",
    "        return None\n",
    "\n",
    "    def get_title_and_first_sentence(self, qword):\n",
    "        return self.engine.whats_new(qword)\n",
    "\n",
    "    def top_entity_pos(self, item, most_c=10):\n",
    "        #search for item.\n",
    "            #for i in feed. if i == feed:\n",
    "        #create a list of words that are close to word in proximity\n",
    "        #score based on proximity to word.\n",
    "        #documents is already a list\n",
    "        word_freq = Counter()\n",
    "        for i in self.ent_dict2:\n",
    "            #print(self.ent_dict2[i])\n",
    "            for x in self.ent_dict2[i]:\n",
    "                if x[0] == item:\n",
    "                    for z in self.ent_dict2[i]:\n",
    "                        if x[0] != z[0]:\n",
    "                            #print((abs(x[1]-z[1])))\n",
    "                            word_freq[z[0]] += 1/(abs((x[1]-z[1])))\n",
    "\n",
    "        return word_freq.most_common(most_c)\n",
    "\n",
    "    def top_entity_dict(self, item, most_c=10):\n",
    "        #documents is already a list\n",
    "        #turn each list into a counter, add all counters together.\n",
    "        mega_counter = Counter()\n",
    "        for i in self.ent_dict:\n",
    "            #get list of counters etc\n",
    "            if item in self.ent_dict[i]:\n",
    "                c = Counter(self.ent_dict[i])\n",
    "                del c[item]\n",
    "                mega_counter += c\n",
    "        return mega_counter.most_common(most_c)\n",
    "\n",
    "    def add_File_Database(self, pickle_path):\n",
    "        p = pickle.load(open(pickle_path, \"rb\"))\n",
    "        self.engine.upload_vd(pickle_path)\n",
    "        # pickle_path example: \"C:\\\\Users\\\\User\\\\Desktop\\\\beaver\\\\NumberWon\\\\numberwon\\\\entity\\\\test.pickle\"\n",
    "        self.ent_dict = self.entize(p, self.ent_dict)\n",
    "        self.ent_dict2 = self.entize2(p, self.ent_dict2)\n",
    "\n",
    "    def add_Folder_Database(self, path_pickle_folder):\n",
    "        #have paths in the form '/path/to/dir'\n",
    "        import os\n",
    "        for f in os.listdir(path_pickle_folder):\n",
    "            if f.endswith(\".pickle\"):\n",
    "                p = pickle.load(open(path_pickle_folder + \"/\" + f, \"rb\"))\n",
    "                self.engine.upload_vd(path_pickle_folder + \"/\" + f)\n",
    "                self.ent_dict = self.entize(p, self.ent_dict)\n",
    "                self.ent_dict2 = self.entize2(p, self.ent_dict2)\n",
    "                print(f, type(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edatb = entityDatabase()\n",
    "#edatb.ent_dict\n",
    "edatb.add_Folder_Database('C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/search/pickles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U.S., Trump, Russian, United States, Russia'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = edatb.top_entity_dict(\"Donald Trump\", most_c=5)\n",
    "listing = [tupling[0] for tupling in c]\n",
    "\", \".join(listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/search')\n",
    "print(\"done\")\n",
    "from entityDatabase import entityDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/profiles/Profiles')\n",
    "from Profile import Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Profile(\"Melanie\")\n",
    "p.pref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'Megan': <profiles.Profiles.Profile.Profile object at 0x00000000060F2AC8>, 'Johnny': <profiles.Profiles.Profile.Profile object at 0x00000000060D7BE0>, 'Melanie': <profiles.Profiles.Profile.Profile object at 0x00000000060D7898>}, dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "npy = np.load('profiles_test_database.npy')\n",
    "npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entertainment']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = UserDatabase('profiles_test_database.npy')\n",
    "d.get_user_by_name('Megan').pref_dict['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from UserDatabase import UserDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists:\n",
      "\tc:\\users\\user\\desktop\\beaver\\dlibmodels\\dlib_models\\dlib_face_recognition_resnet_model_v1.dat\n",
      "File already exists:\n",
      "\tc:\\users\\user\\desktop\\beaver\\dlibmodels\\dlib_models\\shape_predictor_68_face_landmarks.dat\n"
     ]
    }
   ],
   "source": [
    "from fanfiction import Fanfiction\n",
    "from profile_skills import update_current_user\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/profiles/Profiles')\n",
    "from Profile import Profile\n",
    "from UserDatabase import UserDatabase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_ask.models.statement at 0x8fa24e0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.insert(0, 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills')\n",
    "from profile_skills import *\n",
    "\n",
    "yes_intent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/search')\n",
    "from entityDatabase import entityDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = entityDatabase()\n",
    "edatb = entityDatabase()\n",
    "edatb.add_Folder_Database('C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/search/pickles')\n",
    "c = edatb.top_entity_dict(EntityName, most_c=5)\n",
    "listing = [tupling[0] for tupling in c]\n",
    "print(\", \".join(listing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
