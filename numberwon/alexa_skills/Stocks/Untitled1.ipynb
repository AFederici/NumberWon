{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(str(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3610208034515381\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "import sys\n",
    "sys.path.insert(0, \"C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/profiles/Profiles\")\n",
    "from Profile import Profile\n",
    "from UserDatabase import UserDatabase\n",
    "\n",
    "sys.path.insert(0, \"C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills\")\n",
    "from fanfiction import Fanfiction\n",
    "\n",
    "f = Fanfiction()\n",
    "term = \"hunger games\"\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.23454213142395\n"
     ]
    }
   ],
   "source": [
    "t3 = time.time()\n",
    "text = f.find_fanfiction(term)\n",
    "\n",
    "# for key, val in text.items():\n",
    "#     a = open(term + '_test.txt', 'w')\n",
    "#     a.write(val)\n",
    "#     a.close()\n",
    "t4 = time.time()\n",
    "print(t4-t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What?\n",
      "\n",
      "Did I really just hear…?\n",
      "\n",
      "Doris does the honors of turning on the television. \"Ooh, this is so\n",
      "exciting!\" she exclaims. “Remember how she murdered _six_ tributes, she never\n",
      "\n",
      "\n",
      "1.7420132160186768\n",
      "What?\n",
      "\n",
      "Did I really just hear…?\n",
      "\n",
      "Doris does the honors of turning on the television. \"Ooh, this is so\n",
      "exciting!\" she exclaims.\n"
     ]
    }
   ],
   "source": [
    "t5 = time.time()\n",
    "import html2text\n",
    "# with open(term + \"_test.txt\", \"r\") as z:\n",
    "#     t = z.read()\n",
    "#print(type(t))\n",
    "#term = \"harry potter\"\n",
    "text[term] = str(text[term])\n",
    "lm = f.train_lm(text[term], 13)\n",
    "f.text = f.generate_text(lm, 13, nletters=200)\n",
    "f.text = html2text.html2text(f.text)\n",
    "print(f.text)\n",
    "\n",
    "a = open(term + '.txt', 'w')\n",
    "a.write(f.text)\n",
    "a.close()\n",
    "t6 = time.time()\n",
    "print(t6-t5)\n",
    "\n",
    "#html2text.html2text(f.text)\n",
    "print(f.text[:f.text.rfind(\".\")+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for x in tuple([1, 2]):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "gender = \"female\"\n",
    "name_set = \"us\"\n",
    "country = \"us\"\n",
    "site = \"http://www.fakenamegenerator.com\"\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = Request(site, headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "option = soup.find_all(\"option\")\n",
    "\n",
    "#print(\"\\n\".join([option[n].text for n, val in enumerate(option)]))\n",
    "soup.find_all(\"li\", \"lab\")\n",
    "c = soup.find(\"select\", {\"id\": \"c\"})\n",
    "#print(\"ctype\", type(c))\n",
    "gen = soup.find(\"select\", {\"id\": \"gen\"})\n",
    "gender_name = \"female\"\n",
    "l = [\"gen\", \"n\", \"c\"]\n",
    "v = dict()\n",
    "for ele in l:\n",
    "    \"\"\"Parameters:\n",
    "    -------------------------\n",
    "        key: id\n",
    "        returns: problem\"\"\"\n",
    "    link_option = soup.find(\"select\", {\"id\": ele}).find_all(\"option\")\n",
    "    option_value = [opt[\"value\"] for opt in link_option if gender_name.lower() == opt.text.lower()] #to be inputted into url\n",
    "    #session.attributes[\"Gender\"] = option_value[0]\n",
    "    #session.attributes[\"gen\"] = [* soup.find_all(\"option\").text if gender_name in \n",
    "    value = [x.text for x in link_option]\n",
    "    v[ele] = \"\\n\".join(value) #splits page into gender, name_set, country options\n",
    "    \n",
    "# #iterate through v to get options to display in Alexa Card\n",
    "\n",
    "\n",
    "# option[5].text\n",
    "# print([opt[\"value\"] for opt in option if gender_name == opt.text.lower()])\n",
    "\n",
    "\n",
    "print(type(v[\"gen\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jimmie J. Baily'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding_name = soup.find_all(\"div\", \"content\")\n",
    "soup.find(attrs = {\"class\": \"info\", \"class\": \"address\"}).find(\"h3\").text\n",
    "#[ele.text for ele in finding_name if ele.find_all(\"info\") != None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('megan', <profiles.Profiles.Profile.Profile object at 0x000000000769E5C0>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/profiles/Profiles')\n",
    "from Profile import Profile\n",
    "from UserDatabase import UserDatabase\n",
    "\n",
    "alexa_path = 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/profiles'\n",
    "#sys.path.insert(0, alexa_path)\n",
    "#every Fanfiction instance saves a text file into of top fanfiction for a term\n",
    "#iterates through all the databases and saves txt files based on preferences\n",
    "import os\n",
    "save_path = 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/fanfiction_files'\n",
    "import itertools\n",
    "\n",
    "f = Fanfiction()\n",
    "together = list()\n",
    "for f in os.listdir(alexa_path):\n",
    "    if f.endswith(\".npy\"):\n",
    "        #print(f)\n",
    "        d = UserDatabase(alexa_path+\"/\"+f)\n",
    "        print(d)\n",
    "        #find all keys that have fan in them\n",
    "        #l = print([type(val) for key, val in d.dict.items()])\n",
    "        profiles = [val2 for key, val in d.dict.items() for key1, val2 in val.pref_dict.items() if \"fan\" in key1]\n",
    "        #print(profiles)\n",
    "        together.extend(itertools.chain.from_iterable(profiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harry potter']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter Fanfiction: \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~Gripped tightly in her hand, sweat dripping all\n",
      "over it. \"NOW!\" Harry yelled. \"REDUCTO!\" rang all six of their voices,\n",
      "pointing their wands at them. Death Eaters. \"Where's Sirius?\" Harry asked\n",
      "Hagrid a few questions all through schooling and when she started Hogwarts\n",
      "last year. She was really shy and hardly talked to anyone at the school,\n",
      "paralyzing muggle-borns at random.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, 'C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills')\n",
    "f = Fanfiction()\n",
    "import html2text\n",
    "number = 500\n",
    "content = \"\"\n",
    "for term in together:\n",
    "    with open(\"fanfiction_files/\" + str(term) + \".txt\", \"r\", encoding = 'cp1252') as z:\n",
    "        value = str(z.read())\n",
    "        lm = f.train_lm(value, 13)\n",
    "        text = html2text.html2text(f.generate_text(lm, 13, number))\n",
    "        text = text[:text.rfind(\".\") + 1]\n",
    "        content += term.title() + ' Fanfiction: \\n' + text + \"\\n\\n\"\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harry PotterFanfiction: \\n~~~~~~~~~~~~~~~~Gripped tightly in her hand, sweat dripping all over it.\\n\"NOW!\" Harry yelled.\\n\"REDUCTO!\" rang all six of their voices, pointing their wands at them. \\xa0Death Eaters.\\n\"Where\\'s Sirius?\" Harry asked Hagrid a few questions all through schooling and when she started Hogwarts last year. She was really shy and hardly talked to anyone at the school, paralyzing muggle-borns at random. \\xa0The feeling literally haunted Hermione, but deep in her heart she knew she needed a quill but with her mind clouded by the injustice Hogwarts was currently exhibiting, Hermione dashed towards the bulletin board, where most of the week leading up to the flying lesson reading Quidditch Through the Ages, a fascinating book about the popular wizarding sport and flying in general. \\xa0During breakfast the morning before their class, she was helping the other as the majority of the tryouts.\\n\"Oh, come on, Harry,\" Hermione realized that he\\'d been redeemed and could escape with no one the wiser of his true nature once Hagrid was looking in and brought back books as proof. \\xa0\"Dragons, Hagrid was charged for the attacks, as long as he stopped attacking right after the expulsion? \\xa0But she didn\\'t mind - hers were the same. \\xa0They kissed for a minute, then scrapped his drawing. \"Alright. Ruin my dream of being an artist. Hey, what are you talking about?” he shouted, backing away from her with Ron Weasley, the redheaded boy who had made up that lousy spell to use on his rat. \\xa0She noticed Draco Malfoy was up t\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomlist = []\n",
    "len(randomlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.attributes[\"Current_User\"] = session.attributes[\"Current_User\"].lower()\n",
    "p = d.dict[session.attributes[\"Current_User\"]]\n",
    "listing = tuple(p.get_preferences_by_user(session.attributes[\"Current_User\"], key) for key, val in p.pref_dict if \"fan\" in key)\n",
    "listing = itertools.chain.from_iterable(listing)\n",
    "print(\"preferences: \", listing)\n",
    "content = \"\"\n",
    "\n",
    "for term in listing:\n",
    "    with open(\"fanfiction_files/\" + str(term) + \".txt\", \"r\", 'cp1252') as z:\n",
    "        value = str(z.read())\n",
    "        lm = f.train_lm(value, 13)\n",
    "        text = html2text.html2text(f.generate_text(lm, 13, number))\n",
    "        text = text[:text.rfind(\".\") + 1]\n",
    "        content += term.title() + 'Fanfiction: \\n' + value + \"\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "path_pkl = \"C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/search/pickles\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl = pickle.load(open(path_pkl+\"/arts.pickle\", \"rb\"))\n",
    "for key, val in pkl.items():\n",
    "    tokens = nltk.word_tokenize(val)\n",
    "    pos = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Samples', 'NNS'),\n",
       " ('taken', 'VBN'),\n",
       " ('from', 'IN'),\n",
       " ('Salvador', 'NNP'),\n",
       " ('Dali', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('body', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('resolve', 'VB'),\n",
       " ('paternity', 'NN'),\n",
       " ('suit', 'NN'),\n",
       " ('2', 'CD'),\n",
       " ('Min', 'NNP'),\n",
       " ('Read', 'NNP'),\n",
       " ('Workers', 'NNP'),\n",
       " ('carry', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('coffin', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('used', 'VBN'),\n",
       " ('during', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('exhumation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Spanish', 'JJ'),\n",
       " ('artist', 'NN'),\n",
       " ('Salvador', 'NNP'),\n",
       " ('Dali', 'NNP'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('order', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('DNA', 'NNP'),\n",
       " ('samples', 'NNS'),\n",
       " ('following', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('paternity', 'NN'),\n",
       " ('claim', 'NN'),\n",
       " (',', ','),\n",
       " ('at', 'IN'),\n",
       " ('Teatre-Museu', 'NNP'),\n",
       " ('Dali', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Theater-Museum', 'NNP'),\n",
       " ('Dali', 'NNP'),\n",
       " (')', ')'),\n",
       " ('in', 'IN'),\n",
       " ('Figueras', 'NNP'),\n",
       " ('city', 'NN'),\n",
       " (',', ','),\n",
       " ('Spain', 'NNP'),\n",
       " (',', ','),\n",
       " ('July', 'NNP'),\n",
       " ('20', 'CD'),\n",
       " (',', ','),\n",
       " ('2017.Albert', 'CD'),\n",
       " ('Gea', 'NNP'),\n",
       " ('Maria', 'NNP'),\n",
       " ('Pilar', 'NNP'),\n",
       " ('Abel', 'NNP'),\n",
       " (',', ','),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('born', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('1956', 'CD'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('northern', 'JJ'),\n",
       " ('Spanish', 'JJ'),\n",
       " ('town', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Figueras', 'NNP'),\n",
       " ('-', ':'),\n",
       " ('Dali', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('home', 'NN'),\n",
       " ('town', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('place', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('buried', 'VBN'),\n",
       " ('-', ':'),\n",
       " ('claims', 'VBZ'),\n",
       " ('her', 'PRP$'),\n",
       " ('mother', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('an', 'DT'),\n",
       " ('affair', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('painter', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('has', 'VBZ'),\n",
       " ('been', 'VBN'),\n",
       " ('trying', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('prove', 'VB'),\n",
       " ('she', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('his', 'PRP$'),\n",
       " ('daughter', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('years', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Slideshow', 'NNP'),\n",
       " ('(', '('),\n",
       " ('9', 'CD'),\n",
       " ('Images', 'NNS'),\n",
       " (')', ')'),\n",
       " ('When', 'WRB'),\n",
       " ('Dali', 'NNP'),\n",
       " ('died', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('1989', 'CD'),\n",
       " (',', ','),\n",
       " ('aged', 'VBD'),\n",
       " ('84', 'CD'),\n",
       " (',', ','),\n",
       " ('his', 'PRP$'),\n",
       " ('body', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('embalmed', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Narcis', 'NNP'),\n",
       " ('Bardalet', 'NNP'),\n",
       " (',', ','),\n",
       " ('who', 'WP'),\n",
       " ('told', 'VBD'),\n",
       " ('Reuters', 'NNP'),\n",
       " ('that', 'IN'),\n",
       " ('attempts', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('extract', 'VB'),\n",
       " ('DNA', 'NNP'),\n",
       " ('were', 'VBD'),\n",
       " ('likely', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('successful', 'JJ'),\n",
       " (',', ','),\n",
       " ('though', 'IN'),\n",
       " ('``', '``'),\n",
       " ('there', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('also', 'RB'),\n",
       " ('difficulties', 'NNS'),\n",
       " ('because', 'IN'),\n",
       " ('(', '('),\n",
       " ('the', 'DT'),\n",
       " ('body', 'NN'),\n",
       " (')', ')'),\n",
       " ('has', 'VBZ'),\n",
       " ('been', 'VBN'),\n",
       " ('embalmed', 'VBN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('formaldehyde', 'NN'),\n",
       " ('could', 'MD'),\n",
       " ('have', 'VB'),\n",
       " ('damaged', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('nucleus', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('cells', 'NNS'),\n",
       " ('.', '.'),\n",
       " (\"''\", \"''\"),\n",
       " ('``', '``'),\n",
       " ('Getting', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('samples', 'NNS'),\n",
       " (',', ','),\n",
       " ('that', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " (',', ','),\n",
       " ('molars', 'NNS'),\n",
       " (',', ','),\n",
       " ('teeth', 'NNS'),\n",
       " (',', ','),\n",
       " ('long', 'JJ'),\n",
       " ('bones', 'NNS'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('order', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('extract', 'VB'),\n",
       " ('DNA', 'NNP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('easy', 'JJ'),\n",
       " (',', ','),\n",
       " ('because', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('body', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('relatively', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('condition', 'NN'),\n",
       " (',', ','),\n",
       " (\"''\", \"''\"),\n",
       " ('Bardalet', 'NNP'),\n",
       " ('said', 'VBD'),\n",
       " ('.', '.'),\n",
       " ('Dali', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('remains', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('interred', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('crypt', 'NN'),\n",
       " ('under', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('stage', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('domed', 'JJ'),\n",
       " ('Theatre-Museum', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Figueras', 'NNP'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('houses', 'NNS'),\n",
       " ('some', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('art', 'NN'),\n",
       " ('works', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('paintings', 'NNS'),\n",
       " ('he', 'PRP'),\n",
       " ('collected', 'VBD'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('court', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('samples', 'NNS'),\n",
       " ('be', 'VB'),\n",
       " ('taken', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('June', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('may', 'MD'),\n",
       " ('take', 'VB'),\n",
       " ('weeks', 'NNS'),\n",
       " ('before', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('results', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('DNA', 'NN'),\n",
       " ('tests', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('known', 'VBN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8155b020f974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhelp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupenn_tagset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NN')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Samples', 'NNS')\n",
      "['Samples']\n",
      "('taken', 'VBN')\n",
      "['Samples']\n",
      "('from', 'IN')\n",
      "['Samples']\n",
      "('Salvador', 'NNP')\n",
      "['Samples']\n",
      "('Dali', 'NNP')\n",
      "['Samples']\n",
      "(\"'s\", 'POS')\n",
      "['Samples']\n",
      "('body', 'NN')\n",
      "['Samples']\n",
      "('to', 'TO')\n",
      "['Samples']\n",
      "('resolve', 'VB')\n",
      "['Samples']\n",
      "('paternity', 'NN')\n",
      "['Samples']\n",
      "('suit', 'NN')\n",
      "['Samples']\n",
      "('2', 'CD')\n",
      "['Samples']\n",
      "('Min', 'NNP')\n",
      "['Samples']\n",
      "('Read', 'NNP')\n",
      "['Samples']\n",
      "('Workers', 'NNP')\n",
      "['Samples']\n",
      "('carry', 'VBP')\n",
      "['Samples']\n",
      "('a', 'DT')\n",
      "['Samples']\n",
      "('coffin', 'NN')\n",
      "['Samples']\n",
      "('to', 'TO')\n",
      "['Samples']\n",
      "('be', 'VB')\n",
      "['Samples']\n",
      "('used', 'VBN')\n",
      "['Samples']\n",
      "('during', 'IN')\n",
      "['Samples']\n",
      "('the', 'DT')\n",
      "['Samples']\n",
      "('exhumation', 'NN')\n",
      "['Samples']\n",
      "('of', 'IN')\n",
      "['Samples']\n",
      "('Spanish', 'JJ')\n",
      "['Samples']\n",
      "('artist', 'NN')\n",
      "['Samples']\n",
      "('Salvador', 'NNP')\n",
      "['Samples']\n",
      "('Dali', 'NNP')\n",
      "['Samples']\n",
      "(',', ',')\n",
      "['Samples']\n",
      "('in', 'IN')\n",
      "['Samples']\n",
      "('order', 'NN')\n",
      "['Samples']\n",
      "('to', 'TO')\n",
      "['Samples']\n",
      "('obtain', 'VB')\n",
      "['Samples']\n",
      "('DNA', 'NNP')\n",
      "['Samples']\n",
      "('samples', 'NNS')\n",
      "['Samples', 'samples']\n",
      "('following', 'VBG')\n",
      "['Samples', 'samples']\n",
      "('a', 'DT')\n",
      "['Samples', 'samples']\n",
      "('paternity', 'NN')\n",
      "['Samples', 'samples']\n",
      "('claim', 'NN')\n",
      "['Samples', 'samples']\n",
      "(',', ',')\n",
      "['Samples', 'samples']\n",
      "('at', 'IN')\n",
      "['Samples', 'samples']\n",
      "('Teatre-Museu', 'NNP')\n",
      "['Samples', 'samples']\n",
      "('Dali', 'NNP')\n",
      "['Samples', 'samples']\n",
      "('(', '(')\n",
      "['Samples', 'samples']\n",
      "('Theater-Museum', 'NNP')\n",
      "['Samples', 'samples']\n",
      "('Dali', 'NNP')\n",
      "['Samples', 'samples']\n",
      "(')', ')')\n",
      "['Samples', 'samples']\n",
      "('in', 'IN')\n",
      "['Samples', 'samples']\n",
      "('Figueras', 'NNP')\n",
      "['Samples', 'samples']\n",
      "('city', 'NN')\n",
      "['Samples', 'samples']\n",
      "(',', ',')\n",
      "['Samples', 'samples']\n",
      "('Spain', 'NNP')\n",
      "['Samples', 'samples']\n",
      "(',', ',')\n",
      "['Samples', 'samples']\n",
      "('July', 'NNP')\n",
      "['Samples', 'samples']\n",
      "('20', 'CD')\n",
      "['Samples', 'samples']\n",
      "(',', ',')\n",
      "['Samples', 'samples']\n",
      "('2017.Albert', 'CD')\n",
      "['Samples', 'samples']\n",
      "('Gea', 'NNP')\n",
      "['Samples', 'samples']\n",
      "('Maria', 'NNP')\n",
      "['Samples', 'samples']\n",
      "('Pilar', 'NNP')\n",
      "['Samples', 'samples']\n",
      "('Abel', 'NNP')\n",
      "['Samples', 'samples']\n",
      "(',', ',')\n",
      "['Samples', 'samples']\n",
      "('who', 'WP')\n",
      "['Samples', 'samples']\n",
      "('was', 'VBD')\n",
      "['Samples', 'samples']\n",
      "('born', 'VBN')\n",
      "['Samples', 'samples']\n",
      "('in', 'IN')\n",
      "['Samples', 'samples']\n",
      "('1956', 'CD')\n",
      "['Samples', 'samples']\n",
      "('in', 'IN')\n",
      "['Samples', 'samples']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples']\n",
      "('northern', 'JJ')\n",
      "['Samples', 'samples']\n",
      "('Spanish', 'JJ')\n",
      "['Samples', 'samples']\n",
      "('town', 'NN')\n",
      "['Samples', 'samples']\n",
      "('of', 'IN')\n",
      "['Samples', 'samples']\n",
      "('Figueras', 'NNP')\n",
      "['Samples', 'samples']\n",
      "('-', ':')\n",
      "['Samples', 'samples']\n",
      "('Dali', 'NNP')\n",
      "['Samples', 'samples']\n",
      "(\"'s\", 'POS')\n",
      "['Samples', 'samples']\n",
      "('home', 'NN')\n",
      "['Samples', 'samples']\n",
      "('town', 'NN')\n",
      "['Samples', 'samples']\n",
      "('and', 'CC')\n",
      "['Samples', 'samples']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples']\n",
      "('place', 'NN')\n",
      "['Samples', 'samples']\n",
      "('he', 'PRP')\n",
      "['Samples', 'samples']\n",
      "('is', 'VBZ')\n",
      "['Samples', 'samples']\n",
      "('buried', 'VBN')\n",
      "['Samples', 'samples']\n",
      "('-', ':')\n",
      "['Samples', 'samples']\n",
      "('claims', 'VBZ')\n",
      "['Samples', 'samples']\n",
      "('her', 'PRP$')\n",
      "['Samples', 'samples']\n",
      "('mother', 'NN')\n",
      "['Samples', 'samples']\n",
      "('had', 'VBD')\n",
      "['Samples', 'samples']\n",
      "('an', 'DT')\n",
      "['Samples', 'samples']\n",
      "('affair', 'NN')\n",
      "['Samples', 'samples']\n",
      "('with', 'IN')\n",
      "['Samples', 'samples']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples']\n",
      "('painter', 'NN')\n",
      "['Samples', 'samples']\n",
      "('and', 'CC')\n",
      "['Samples', 'samples']\n",
      "('has', 'VBZ')\n",
      "['Samples', 'samples']\n",
      "('been', 'VBN')\n",
      "['Samples', 'samples']\n",
      "('trying', 'VBG')\n",
      "['Samples', 'samples']\n",
      "('to', 'TO')\n",
      "['Samples', 'samples']\n",
      "('prove', 'VB')\n",
      "['Samples', 'samples']\n",
      "('she', 'PRP')\n",
      "['Samples', 'samples']\n",
      "('is', 'VBZ')\n",
      "['Samples', 'samples']\n",
      "('his', 'PRP$')\n",
      "['Samples', 'samples']\n",
      "('daughter', 'NN')\n",
      "['Samples', 'samples']\n",
      "('for', 'IN')\n",
      "['Samples', 'samples']\n",
      "('years', 'NNS')\n",
      "['Samples', 'samples', 'years']\n",
      "('.', '.')\n",
      "['Samples', 'samples', 'years']\n",
      "('Slideshow', 'NNP')\n",
      "['Samples', 'samples', 'years']\n",
      "('(', '(')\n",
      "['Samples', 'samples', 'years']\n",
      "('9', 'CD')\n",
      "['Samples', 'samples', 'years']\n",
      "('Images', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "(')', ')')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('When', 'WRB')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('Dali', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('died', 'VBD')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('in', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('1989', 'CD')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('aged', 'VBD')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('84', 'CD')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('his', 'PRP$')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('body', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('was', 'VBD')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('embalmed', 'VBN')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('by', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('Narcis', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('Bardalet', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('who', 'WP')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('told', 'VBD')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('Reuters', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('that', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('attempts', 'VBZ')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('to', 'TO')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('extract', 'VB')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('DNA', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('were', 'VBD')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('likely', 'JJ')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('to', 'TO')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('be', 'VB')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('successful', 'JJ')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('though', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('``', '``')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('there', 'EX')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('are', 'VBP')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('also', 'RB')\n",
      "['Samples', 'samples', 'years', 'Images']\n",
      "('difficulties', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('because', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('(', '(')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('body', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "(')', ')')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('has', 'VBZ')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('been', 'VBN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('embalmed', 'VBN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('and', 'CC')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('formaldehyde', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('could', 'MD')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('have', 'VB')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('damaged', 'VBN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('nucleus', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('of', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties']\n",
      "('cells', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells']\n",
      "('.', '.')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells']\n",
      "(\"''\", \"''\")\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells']\n",
      "('``', '``')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells']\n",
      "('Getting', 'VBG')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells']\n",
      "('samples', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples']\n",
      "('that', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples']\n",
      "('is', 'VBZ')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples']\n",
      "('molars', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars']\n",
      "('teeth', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth']\n",
      "('long', 'JJ')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth']\n",
      "('bones', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('in', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('order', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('to', 'TO')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('extract', 'VB')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('DNA', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('will', 'MD')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('be', 'VB')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('easy', 'JJ')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('because', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('body', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('will', 'MD')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('be', 'VB')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('in', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('a', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('relatively', 'RB')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('good', 'JJ')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('condition', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "(\"''\", \"''\")\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('Bardalet', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('said', 'VBD')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('.', '.')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('Dali', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "(\"'s\", 'POS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones']\n",
      "('remains', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('are', 'VBP')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('interred', 'VBN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('in', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('a', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('crypt', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('under', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('stage', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('of', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('domed', 'JJ')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('Theatre-Museum', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('in', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('Figueras', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "(',', ',')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('which', 'WDT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains']\n",
      "('houses', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses']\n",
      "('some', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses']\n",
      "('of', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses']\n",
      "('his', 'PRP$')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses']\n",
      "('art', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses']\n",
      "('works', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works']\n",
      "('and', 'CC')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works']\n",
      "('paintings', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings']\n",
      "('he', 'PRP')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings']\n",
      "('collected', 'VBD')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings']\n",
      "('.', '.')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings']\n",
      "('The', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings']\n",
      "('court', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings']\n",
      "('samples', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples']\n",
      "('be', 'VB')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples']\n",
      "('taken', 'VBN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples']\n",
      "('in', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples']\n",
      "('June', 'NNP')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples']\n",
      "('.', '.')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples']\n",
      "('It', 'PRP')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples']\n",
      "('may', 'MD')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples']\n",
      "('take', 'VB')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples']\n",
      "('weeks', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks']\n",
      "('before', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks']\n",
      "('results', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks', 'results']\n",
      "('of', 'IN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks', 'results']\n",
      "('the', 'DT')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks', 'results']\n",
      "('DNA', 'NN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks', 'results']\n",
      "('tests', 'NNS')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks', 'results', 'tests']\n",
      "('are', 'VBP')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks', 'results', 'tests']\n",
      "('known', 'VBN')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks', 'results', 'tests']\n",
      "('.', '.')\n",
      "['Samples', 'samples', 'years', 'Images', 'difficulties', 'cells', 'samples', 'molars', 'teeth', 'bones', 'remains', 'houses', 'works', 'paintings', 'samples', 'weeks', 'results', 'tests']\n"
     ]
    }
   ],
   "source": [
    "#The story is about a _adjective _NN which VBZverb-singular NNS-noun plural in order to VBP NNS\n",
    "#It takes place in NNP\n",
    "#The _NN _RB _'VBZ' JJ JJ  NNS\n",
    "NN = []\n",
    "VBZ = []\n",
    "NNS = []\n",
    "VBP = []\n",
    "CD = []\n",
    "RB = []\n",
    "VBZ = []\n",
    "JJ = []\n",
    "\n",
    "all_list = [\"NN\", \"VBZ\", \"NNS\", \"VBP\", \"CD\", \"RB\", \"VBZ\", \"JJ\"]\n",
    "\n",
    "for tup in pos:\n",
    "    sec = tup[1]\n",
    "    if sec in all_list:\n",
    "        exec(\"%s.append('%s')\" % (sec, tup[0]))\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': ('dollar', '$ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$ '),\n",
       " \"''\": ('closing quotation mark', \"' '' \"),\n",
       " '(': ('opening parenthesis', '( [ { '),\n",
       " ')': ('closing parenthesis', ') ] } '),\n",
       " ',': ('comma', ', '),\n",
       " '--': ('dash', '-- '),\n",
       " '.': ('sentence terminator', '. ! ? '),\n",
       " ':': ('colon or ellipsis', ': ; ... '),\n",
       " 'CC': ('conjunction, coordinating',\n",
       "  \"& 'n and both but either et for less minus neither nor or plus so therefore times v. versus vs. whether yet \"),\n",
       " 'CD': ('numeral, cardinal',\n",
       "  \"mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025 fifteen 271,124 dozen quintillion DM2,000 ... \"),\n",
       " 'DT': ('determiner',\n",
       "  'all an another any both del each either every half la many much nary neither no some such that the them these this those '),\n",
       " 'EX': ('existential there', 'there '),\n",
       " 'FW': ('foreign word',\n",
       "  \"gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte terram fiche oui corporis ... \"),\n",
       " 'IN': ('preposition or conjunction, subordinating',\n",
       "  'astride among uppon whether out inside pro despite on by throughout below within for towards near behind atop around if like until below next into if beside ... '),\n",
       " 'JJ': ('adjective or numeral, ordinal',\n",
       "  'third ill-mannered pre-war regrettable oiled calamitous first separable ectoplasmic battery-powered participatory fourth still-to-be-named multilingual multi-disciplinary ... '),\n",
       " 'JJR': ('adjective, comparative',\n",
       "  'bleaker braver breezier briefer brighter brisker broader bumper busier calmer cheaper choosier cleaner clearer closer colder commoner costlier cozier creamier crunchier cuter ... '),\n",
       " 'JJS': ('adjective, superlative',\n",
       "  'calmest cheapest choicest classiest cleanest clearest closest commonest corniest costliest crassest creepiest crudest cutest darkest deadliest dearest deepest densest dinkiest ... '),\n",
       " 'LS': ('list item marker',\n",
       "  'A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005 SP-44007 Second Third Three Two * a b c d first five four one six three two '),\n",
       " 'MD': ('modal auxiliary',\n",
       "  \"can cannot could couldn't dare may might must need ought shall should shouldn't will would \"),\n",
       " 'NN': ('noun, common, singular or mass',\n",
       "  'common-carrier cabbage knuckle-duster Casino afghan shed thermostat investment slide humour falloff slick wind hyena override subhumanity machinist ... '),\n",
       " 'NNP': ('noun, proper, singular',\n",
       "  'Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA Shannon A.K.C. Meltex Liverpool ... '),\n",
       " 'NNPS': ('noun, proper, plural',\n",
       "  'Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques Apache Apaches Apocrypha ... '),\n",
       " 'NNS': ('noun, common, plural',\n",
       "  'undergraduates scotches bric-a-brac products bodyguards facets coasts divestitures storehouses designs clubs fragrances averages subjectivists apprehensions muses factory-jobs ... '),\n",
       " 'PDT': ('pre-determiner', 'all both half many quite such sure this '),\n",
       " 'POS': ('genitive marker', \"' 's \"),\n",
       " 'PRP': ('pronoun, personal',\n",
       "  'hers herself him himself hisself it itself me myself one oneself ours ourselves ownself self she thee theirs them themselves they thou thy us '),\n",
       " 'PRP$': ('pronoun, possessive', 'her his mine my our ours their thy your '),\n",
       " 'RB': ('adverb',\n",
       "  'occasionally unabatingly maddeningly adventurously professedly stirringly prominently technologically magisterially predominately swiftly fiscally pitilessly ... '),\n",
       " 'RBR': ('adverb, comparative',\n",
       "  'further gloomier grander graver greater grimmer harder harsher healthier heavier higher however larger later leaner lengthier less-perfectly lesser lonelier longer louder lower more ... '),\n",
       " 'RBS': ('adverb, superlative',\n",
       "  'best biggest bluntest earliest farthest first furthest hardest heartiest highest largest least less most nearest second tightest worst '),\n",
       " 'RP': ('particle',\n",
       "  'aboard about across along apart around aside at away back before behind by crop down ever fast for forth from go high i.e. in into just later low more off on open out over per pie raising start teeth that through under unto up up-pp upon whole with you '),\n",
       " 'SYM': ('symbol',\n",
       "  \"% & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** *** \"),\n",
       " 'TO': ('\"to\" as preposition or infinitive marker', 'to '),\n",
       " 'UH': ('interjection',\n",
       "  'Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly man baby diddle hush sonuvabitch ... '),\n",
       " 'VB': ('verb, base form',\n",
       "  'ask assemble assess assign assume atone attention avoid bake balkanize bank begin behold believe bend benefit bevel beware bless boil bomb boost brace break bring broil brush build ... '),\n",
       " 'VBD': ('verb, past tense',\n",
       "  'dipped pleaded swiped regummed soaked tidied convened halted registered cushioned exacted snubbed strode aimed adopted belied figgered speculated wore appreciated contemplated ... '),\n",
       " 'VBG': ('verb, present participle or gerund',\n",
       "  \"telegraphing stirring focusing angering judging stalling lactating hankerin' alleging veering capping approaching traveling besieging encrypting interrupting erasing wincing ... \"),\n",
       " 'VBN': ('verb, past participle',\n",
       "  'multihulled dilapidated aerosolized chaired languished panelized used experimented flourished imitated reunifed factored condensed sheared unsettled primed dubbed desired ... '),\n",
       " 'VBP': ('verb, present tense, not 3rd person singular',\n",
       "  'predominate wrap resort sue twist spill cure lengthen brush terminate appear tend stray glisten obtain comprise detest tease attract emphasize mold postpone sever return wag ... '),\n",
       " 'VBZ': ('verb, present tense, 3rd person singular',\n",
       "  'bases reconstructs marks mixes displeases seals carps weaves snatches slumps stretches authorizes smolders pictures emerges stockpiles seduces fizzes uses bolsters slaps speaks pleads ... '),\n",
       " 'WDT': ('WH-determiner', 'that what whatever which whichever '),\n",
       " 'WP': ('WH-pronoun',\n",
       "  'that what whatever whatsoever which who whom whosoever '),\n",
       " 'WP$': ('WH-pronoun, possessive', 'whose '),\n",
       " 'WRB': ('Wh-adverb',\n",
       "  'how however whence whenever where whereby whereever wherein whereof why '),\n",
       " '``': ('opening quotation mark', '` `` ')}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.data import load\n",
    "tagdict = load(\"help/tagsets/\" + \"upenn_tagset\" + \".pickle\")\n",
    "tagdict\n",
    "#The story is about a _adjective _NN which VBZverb-singular NNS-noun plural in order to VBP NNS\n",
    "#It takes place in NNP\n",
    "#The _NN _RB _'VBZ' JJ JJ  NNS\n",
    "\n",
    "import pickle\n",
    "pkl = \n",
    "\n",
    "#the _adjective _noun _verb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print((tagdict['VBN'][1]).replace(\" \", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ExpatError",
     "evalue": "unclosed token: line 1, column 1495",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExpatError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-72bb0a479acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmm_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminidom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmm_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmm_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetElementsByTagName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'node'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\xml\\dom\\minidom.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(file, parser, bufsize)\u001b[0m\n\u001b[1;32m   1956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpulldom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\xml\\dom\\expatbuilder.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(file, namespaces)\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\xml\\dom\\expatbuilder.py\u001b[0m in \u001b[0;36mparseFile\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mfirst_buffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mParseEscape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExpatError\u001b[0m: unclosed token: line 1, column 1495"
     ]
    }
   ],
   "source": [
    "import xml.dom.minidom\n",
    "import random\n",
    "import sys\n",
    "\n",
    "fileName = \"C:/Users/User/Desktop/beaver/NumberWon/numberwon/alexa_skills/fanfiction_files/twilight.txt\"\n",
    "numItems = 15\n",
    "\n",
    "mm_file = open(fileName,'r')\n",
    "mm = xml.dom.minidom.parse(mm_file)\n",
    "\n",
    "mm_nodes = mm.getElementsByTagName('node')\n",
    "\n",
    "textList = []\n",
    "for node in mm_nodes:\n",
    "    textList.append(node.getAttribute('TEXT'))\n",
    "\n",
    "if numItems > len(textList):\n",
    "    print('You have specified more items than exist in the file.\\nPrinting everything in random order.')\n",
    "    numItems = len(textList)\n",
    "else:\n",
    "    print('Printing %s of %s items.' % (numItems,len(textList)))\n",
    "\n",
    "print( '---------------------------------------------------') \n",
    "\n",
    "for i in range(numItems): \n",
    "    print(textList.pop(random.randrange(len(textList))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 4))\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to parse line 2: ... S -> NP VP\nExpected a nonterminal, found: ... S -> NP VP",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mread_grammar\u001b[0;34m(input, nonterm_parser, probabilistic, encoding)\u001b[0m\n\u001b[1;32m   1269\u001b[0m                 \u001b[1;31m# expand out the disjunctions on the RHS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m                 \u001b[0mproductions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_read_production\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonterm_parser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilistic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36m_read_production\u001b[0;34m(line, nonterm_parser, probabilistic)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[1;31m# Parse the left-hand side.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m     \u001b[0mlhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnonterm_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mstandard_nonterm_parser\u001b[0;34m(string, pos)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     if not m: raise ValueError('Expected a nonterminal, found: '\n\u001b[0;32m-> 1286\u001b[0;31m                                + string[pos:])\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mNonterminal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a nonterminal, found: ... S -> NP VP",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6a33d5f108d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m...\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'shot'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m...\u001b[0m \u001b[0mP\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'in'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m ... \"\"\")\n\u001b[0m",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mfromstring\u001b[0;34m(cls, input, encoding)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \"\"\"\n\u001b[1;32m    518\u001b[0m         start, productions = read_grammar(input, standard_nonterm_parser,\n\u001b[0;32m--> 519\u001b[0;31m                                           encoding=encoding)\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproductions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mread_grammar\u001b[0;34m(input, nonterm_parser, probabilistic, encoding)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m             raise ValueError('Unable to parse line %s: %s\\n%s' %\n\u001b[0;32m-> 1273\u001b[0;31m                              (linenum+1, line, e))\n\u001b[0m\u001b[1;32m   1274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mproductions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to parse line 2: ... S -> NP VP\nExpected a nonterminal, found: ... S -> NP VP"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "... S -> NP VP\n",
    "... PP -> P NP\n",
    "... NP -> Det N | Det N PP | 'I'\n",
    "... VP -> V NP | VP PP\n",
    "... Det -> 'an' | 'my'\n",
    "... N -> 'elephant' | 'pajamas'\n",
    "... V -> 'shot'\n",
    "... P -> 'in'\n",
    "... \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
